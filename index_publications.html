<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name=viewport content="width=device-width,initial-scale=1,user-scalable=no">
<!--<meta name=viewport content="width=device-width,initial-scale=1">-->

<!--    RevealJS--->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/blood.css" />

<!--[if gte IE 9]>
<style type="text/css">
.gradient {
filter: none;
}
</style>
<![endif]-->
<link href='https://fonts.googleapis.com/css?family=Raleway:300,400,500&subset=latin' rel='stylesheet' type='text/css'>
<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />
<!--TODO check <style>img:is([sizes="auto" i], [sizes^="auto," i]) { contain-intrinsic-size: 3000px 1500px }</style>-->

<!-- This site is optimized with the Yoast SEO plugin v23.7 - https://yoast.com/wordpress/plugins/seo/ -->
<title>OARL_Web</title>
<meta name="description" content="The project website for our ORAL website." />
<link rel="canonical" href="https://aam-seals.github.io/aam-seals-v1/" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="OARL" />
<meta property="og:description" content="The project website for our ORAL website." />
<meta property="og:url" content="https://aam-seals.github.io/aam-seals-v1/" />
<meta property="og:site_name" content="OARL" />

<link rel="stylesheet" type="text/css" href="./static/css/FromTiger/bnfsg.css" />

<style>

    .btn-3D { position: absolute; top: 20px; right: 20px; }
</style>

<style id='classic-theme-styles-inline-css' type='text/css'>
/*! This file is auto-generated */
.wp-block-button__link{color:#fff;background-color:#32373c;border-radius:9999px;box-shadow:none;text-decoration:none;padding:calc(.667em + 2px) calc(1.333em + 2px);font-size:1.125em}.wp-block-file__button{background:#32373c;color:#fff;text-decoration:none}
</style>
<style id='global-styles-inline-css' type='text/css'>
:root{--wp--preset--aspect-ratio--square: 1;--wp--preset--aspect-ratio--4-3: 4/3;--wp--preset--aspect-ratio--3-4: 3/4;--wp--preset--aspect-ratio--3-2: 3/2;--wp--preset--aspect-ratio--2-3: 2/3;--wp--preset--aspect-ratio--16-9: 16/9;--wp--preset--aspect-ratio--9-16: 9/16;--wp--preset--color--black: #000000;--wp--preset--color--cyan-bluish-gray: #abb8c3;--wp--preset--color--white: #ffffff;--wp--preset--color--pale-pink: #f78da7;--wp--preset--color--vivid-red: #cf2e2e;--wp--preset--color--luminous-vivid-orange: #ff6900;--wp--preset--color--luminous-vivid-amber: #fcb900;--wp--preset--color--light-green-cyan: #7bdcb5;--wp--preset--color--vivid-green-cyan: #00d084;--wp--preset--color--pale-cyan-blue: #8ed1fc;--wp--preset--color--vivid-cyan-blue: #0693e3;--wp--preset--color--vivid-purple: #9b51e0;--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple: linear-gradient(135deg,rgba(6,147,227,1) 0%,rgb(155,81,224) 100%);--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan: linear-gradient(135deg,rgb(122,220,180) 0%,rgb(0,208,130) 100%);--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange: linear-gradient(135deg,rgba(252,185,0,1) 0%,rgba(255,105,0,1) 100%);--wp--preset--gradient--luminous-vivid-orange-to-vivid-red: linear-gradient(135deg,rgba(255,105,0,1) 0%,rgb(207,46,46) 100%);--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray: linear-gradient(135deg,rgb(238,238,238) 0%,rgb(169,184,195) 100%);--wp--preset--gradient--cool-to-warm-spectrum: linear-gradient(135deg,rgb(74,234,220) 0%,rgb(151,120,209) 20%,rgb(207,42,186) 40%,rgb(238,44,130) 60%,rgb(251,105,98) 80%,rgb(254,248,76) 100%);--wp--preset--gradient--blush-light-purple: linear-gradient(135deg,rgb(255,206,236) 0%,rgb(152,150,240) 100%);--wp--preset--gradient--blush-bordeaux: linear-gradient(135deg,rgb(254,205,165) 0%,rgb(254,45,45) 50%,rgb(107,0,62) 100%);--wp--preset--gradient--luminous-dusk: linear-gradient(135deg,rgb(255,203,112) 0%,rgb(199,81,192) 50%,rgb(65,88,208) 100%);--wp--preset--gradient--pale-ocean: linear-gradient(135deg,rgb(255,245,203) 0%,rgb(182,227,212) 50%,rgb(51,167,181) 100%);--wp--preset--gradient--electric-grass: linear-gradient(135deg,rgb(202,248,128) 0%,rgb(113,206,126) 100%);--wp--preset--gradient--midnight: linear-gradient(135deg,rgb(2,3,129) 0%,rgb(40,116,252) 100%);--wp--preset--font-size--small: 13px;--wp--preset--font-size--medium: 20px;--wp--preset--font-size--large: 36px;--wp--preset--font-size--x-large: 42px;--wp--preset--spacing--20: 0.44rem;--wp--preset--spacing--30: 0.67rem;--wp--preset--spacing--40: 1rem;--wp--preset--spacing--50: 1.5rem;--wp--preset--spacing--60: 2.25rem;--wp--preset--spacing--70: 3.38rem;--wp--preset--spacing--80: 5.06rem;--wp--preset--shadow--natural: 6px 6px 9px rgba(0, 0, 0, 0.2);--wp--preset--shadow--deep: 12px 12px 50px rgba(0, 0, 0, 0.4);--wp--preset--shadow--sharp: 6px 6px 0px rgba(0, 0, 0, 0.2);--wp--preset--shadow--outlined: 6px 6px 0px -3px rgba(255, 255, 255, 1), 6px 6px rgba(0, 0, 0, 1);--wp--preset--shadow--crisp: 6px 6px 0px rgba(0, 0, 0, 1);}:where(.is-layout-flex){gap: 0.5em;}:where(.is-layout-grid){gap: 0.5em;}body .is-layout-flex{display: flex;}.is-layout-flex{flex-wrap: wrap;align-items: center;}.is-layout-flex > :is(*, div){margin: 0;}body .is-layout-grid{display: grid;}.is-layout-grid > :is(*, div){margin: 0;}:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}.has-black-color{color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-color{color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-color{color: var(--wp--preset--color--white) !important;}.has-pale-pink-color{color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-color{color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-color{color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-color{color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-color{color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-color{color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-color{color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-color{color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-color{color: var(--wp--preset--color--vivid-purple) !important;}.has-black-background-color{background-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-background-color{background-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-background-color{background-color: var(--wp--preset--color--white) !important;}.has-pale-pink-background-color{background-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-background-color{background-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-background-color{background-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-background-color{background-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-background-color{background-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-background-color{background-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-background-color{background-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-background-color{background-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-background-color{background-color: var(--wp--preset--color--vivid-purple) !important;}.has-black-border-color{border-color: var(--wp--preset--color--black) !important;}.has-cyan-bluish-gray-border-color{border-color: var(--wp--preset--color--cyan-bluish-gray) !important;}.has-white-border-color{border-color: var(--wp--preset--color--white) !important;}.has-pale-pink-border-color{border-color: var(--wp--preset--color--pale-pink) !important;}.has-vivid-red-border-color{border-color: var(--wp--preset--color--vivid-red) !important;}.has-luminous-vivid-orange-border-color{border-color: var(--wp--preset--color--luminous-vivid-orange) !important;}.has-luminous-vivid-amber-border-color{border-color: var(--wp--preset--color--luminous-vivid-amber) !important;}.has-light-green-cyan-border-color{border-color: var(--wp--preset--color--light-green-cyan) !important;}.has-vivid-green-cyan-border-color{border-color: var(--wp--preset--color--vivid-green-cyan) !important;}.has-pale-cyan-blue-border-color{border-color: var(--wp--preset--color--pale-cyan-blue) !important;}.has-vivid-cyan-blue-border-color{border-color: var(--wp--preset--color--vivid-cyan-blue) !important;}.has-vivid-purple-border-color{border-color: var(--wp--preset--color--vivid-purple) !important;}.has-vivid-cyan-blue-to-vivid-purple-gradient-background{background: var(--wp--preset--gradient--vivid-cyan-blue-to-vivid-purple) !important;}.has-light-green-cyan-to-vivid-green-cyan-gradient-background{background: var(--wp--preset--gradient--light-green-cyan-to-vivid-green-cyan) !important;}.has-luminous-vivid-amber-to-luminous-vivid-orange-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-amber-to-luminous-vivid-orange) !important;}.has-luminous-vivid-orange-to-vivid-red-gradient-background{background: var(--wp--preset--gradient--luminous-vivid-orange-to-vivid-red) !important;}.has-very-light-gray-to-cyan-bluish-gray-gradient-background{background: var(--wp--preset--gradient--very-light-gray-to-cyan-bluish-gray) !important;}.has-cool-to-warm-spectrum-gradient-background{background: var(--wp--preset--gradient--cool-to-warm-spectrum) !important;}.has-blush-light-purple-gradient-background{background: var(--wp--preset--gradient--blush-light-purple) !important;}.has-blush-bordeaux-gradient-background{background: var(--wp--preset--gradient--blush-bordeaux) !important;}.has-luminous-dusk-gradient-background{background: var(--wp--preset--gradient--luminous-dusk) !important;}.has-pale-ocean-gradient-background{background: var(--wp--preset--gradient--pale-ocean) !important;}.has-electric-grass-gradient-background{background: var(--wp--preset--gradient--electric-grass) !important;}.has-midnight-gradient-background{background: var(--wp--preset--gradient--midnight) !important;}.has-small-font-size{font-size: var(--wp--preset--font-size--small) !important;}.has-medium-font-size{font-size: var(--wp--preset--font-size--medium) !important;}.has-large-font-size{font-size: var(--wp--preset--font-size--large) !important;}.has-x-large-font-size{font-size: var(--wp--preset--font-size--x-large) !important;}
:where(.wp-block-post-template.is-layout-flex){gap: 1.25em;}:where(.wp-block-post-template.is-layout-grid){gap: 1.25em;}
:where(.wp-block-columns.is-layout-flex){gap: 2em;}:where(.wp-block-columns.is-layout-grid){gap: 2em;}
:root :where(.wp-block-pullquote){font-size: 1.5em;line-height: 1.6;}
</style>

<link rel="stylesheet" type="text/css" href="./static/css/FromTiger/bnftz.css"  media="all"/>
<script src='./static/js/FromTiger/bnfsg.js' type="text/javascript"></script>
<script data-cfasync="false" data-wpfc-render="false" type="text/javascript" id='monsterinsights-frontend-script-js-extra'>/* <![CDATA[ */
var monsterinsights_frontend = {"js_events_tracking":"true","download_extensions":"doc,pdf,ppt,zip,xls,docx,pptx,xlsx","inbound_paths":"[{\"path\":\"\\\/go\\\/\",\"label\":\"affiliate\"},{\"path\":\"\\\/recommend\\\/\",\"label\":\"affiliate\"}]","home_url":"https:\/\/tigerhousefilms.com","hash_tracking":"false","v4_id":"G-7DEP89XTJ2"};/* ]]> */
</script>
<script src='./static/js/FromTiger/bnfsg_1.js' type="text/javascript"></script>

<!--Our CSS  -->
<link rel="stylesheet" href="./static/css/index.css">
<link rel="stylesheet" href="./static/css/window-carousel.css">
<link rel="stylesheet" href="./static/css/tv-effect.css">
<link rel="stylesheet" href="./static/css/students.css">
<link rel="stylesheet" href="./static/css/publication.css">
<script src="./static/js/tv-effect.js" type="text/javascript"></script>


<!--[if lte IE 9]>
<link rel="stylesheet" type="text/css" href="./static/css/FromTiger/vc_lte_ie9.min.css" media="screen"><![endif]--><style type="text/css">.broken_link, a.broken_link {
text-decoration: line-through;
}</style><noscript><style type="text/css"> .wpb_animate_when_almost_visible { opacity: 1; }</style></noscript>
</head>
<body class="not-home-page page-template page-template-full_width page-template-full_width-php page page-id-20996  wpb-js-composer js-comp-ver-5.7 vc_responsive">
<div class="ajax_loader"><div class="ajax_loader_1"><div class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div></div></div>

<section class="side_menu right">
<h2 class="side_menu_title">News</h2>
<div class="side_menu_scroll">
    <p>
      <strong style="color: white">May 16, 2025:</strong> Dr. Zha delivered a keynote talk at the NSF Data Science Symposium, hosted by North Carolina A&T State University. Talk Title:
      <a href="YOUR_LINK_HERE" target="_blank">
        <i>Embodied and Human-Aligned Intelligence across Air, Water, and Land: From Simulation to Data-Efficient Learning</i>
      </a>
    </p>

    <p>
      <strong style="color: white">Feb 7, 2025:</strong> William Yang and Dr. Zha were interviewed by the University of Maryland about our
      <a href="https://aam-seals.umd.edu/" target="_blank">Aerial-Aquatic Manipulators project</a>.
    </p>

    <p>
      <strong style="color: white">Dec 2, 2024:</strong> Our paper
      <a href="https://www.snehesh.com/natsgld/" target="_blank">
        <i>“NatSGLD: A Dataset with <u>S</u>peech, <u>G</u>estures, <u>L</u>ogic, and <u>D</u>emonstrations for Robot Learning in <u>Nat</u>ural Human-Robot Interaction”</i>
      </a>
      is accepted by HRI-25.
    </p>

    <p>
      <strong style="color: white">Aug 22, 2024:</strong> Dr. Zha gave a faculty talk at the
      <a href="https://www.cs.umd.edu/community/research-day" target="_blank">Maryland Research Day</a> event.
    </p>

    <p>
      <strong style="color: white">July 10, 2024:</strong> Our paper
      <a href="https://guansuns.github.io/pages/vlm-critic/" target="_blank">
        “Task Success” is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors
      </a>
      is accepted by COLM-24.
    </p>

    <p>
      <strong style="color: white">July 3, 2024:</strong> Dr. Zha gave an invited talk at the Robotics Institute at Carnegie Mellon University:
      <a href="" target="_blank">
        “Cognitively-Enhanced Robotic Manipulation across Sea, Air, and Land”
      </a>.
    </p>

    <p>
      <strong style="color: white">May 23, 2024:</strong> Dr. Zha delivered a faculty talk at the
      <a href="https://robotics.umd.edu/symposium2024" target="_blank">Maryland Robotics Center Research Symposium 2024</a>.
      Watch the talk here: <a href="https://www.youtube.com/watch?v=8ekZpiXP2do" target="_blank">Video</a>.
    </p>

    <p>
      <strong style="color: white">Dec 9, 2023:</strong> Our paper
      <a href="https://github.com/YantianZha/SERLfD" target="_blank">Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning</a>
      is accepted by AAAI-24 Main Track. Thanks to my collaborators!
    </p>

    <p>
  <strong style="color: white">July 2023:</strong> Excited to announce our workshop
      <a href="https://yantianzha.github.io/crl.github.io/" target="_blank">
        Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions
      </a>
      at CoRL-23.
    </p>

    <!-- add more news items here -->

    <div style="height: 30px;"></div> <!-- extra scroll space -->
</div>
</section>

<div class="wrapper">
<div class="wrapper_inner">
<!-- Google Analytics start -->
<!-- Google Analytics end -->
<header class="page_header   fixed  light ">
<div class="header_inner clearfix">
<div class="header_top_bottom_holder">
<div class="header_bottom clearfix" style='' >

<div class="header_inner_left">
<div class="mobile_menu_button"><span><i class="fa fa-bars"></i></span></div>
<div class="logo_wrapper">
<div class="q_logo"><a href="index.html"><img class="normal" src="media/figures/logo/OARL_Logo_FFFF2.png" alt="Logo"/><img class="light" src="media/figures/logo/OARL_Logo_FFFF2.png" alt="Logo"/><img class="dark" src="media/figures/logo/OARL_Logo_white.png" alt="Logo"/><img class="sticky" src="media/figures/logo/OARL_Logo_FFFF2.png" alt="Logo"/></a></div>
<!--<div class="q_logo"><a href="https://tigerhousefilms.com/"><img class="normal" src="media/figures/logo/OARL_Logo_Horiz.png" alt="Logo"/><img class="light" src="media/figures/logo/OARL_Logo_Horiz.png" alt="Logo"/><img class="dark" src="media/figures/logo/OARL_Logo_HorizWhite.png" alt="Logo"/><img class="sticky" src="media/figures/logo/OARL_Logo_Horiz.png" alt="Logo"/></a></div>-->
</div>
</div>

<div class="header_inner_right">
<div class="side_menu_button_wrapper right">
<div class="side_menu_button">
<a class="side_menu_button_link" href="javascript:void(0)">
<i class="fa fa-bars"></i>
</a>                                    </div>
</div>
</div>
<nav class="main_menu drop_down right">
<ul id="menu-top-menu-2024" class="">

<li id="nav-menu-item-20915" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children  has_sub narrow"><a href="index.html" class=""><i class="menu_icon fa blank"></i><span style="color: black">Home</span><span class="line"></span></a>
<div class="second bellow_header"><div class="inner"><div class="inner_arrow"></div><ul>
<li id="nav-menu-item-20916" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index.html#main-pg" class=""><i class="menu_icon fa blank"></i><span>Main Page</span><span class="line"></span></a></li>
</ul></div></div>
</li>

<li id="nav-menu-item-20925" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children  has_sub narrow"><a href="index_publications.html" class=""><i class="menu_icon fa blank"></i><span style="color: black">Research</span><span class="line"></span></a>
<div class="second bellow_header"><div class="inner"><div class="inner_arrow"></div><ul>
<!--<li id="nav-menu-item-20926" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="https://arxiv.org/abs/2412.19744" target="_blank" class=""><i class="menu_icon fa blank"></i><span>Projects</span><span class="line"></span></a></li>-->
<li id="nav-menu-item-20927" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_publications.html" class=""><i class="menu_icon fa blank"></i><span>Publications</span><span class="line"></span></a></li>
<li id="nav-menu-item-20928" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="" class=""><i class="menu_icon fa blank"></i><span>Equipment</span><span class="line"></span></a></li>
</ul></div></div>
</li>

<li id="nav-menu-item-20930" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children  has_sub narrow"><a href="index_people.html" class=""><i class="menu_icon fa blank"></i><span style="color: black">People</span><span class="line"></span></a>
<div class="second bellow_header"><div class="inner"><div class="inner_arrow"></div><ul>
<li id="nav-menu-item-20933" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#director" class=""><i class="menu_icon fa blank"></i><span>Director</span><span class="line"></span></a></li>
<li id="nav-menu-item-20934" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#postdocs" class=""><i class="menu_icon fa blank"></i><span>Post-Docs</span><span class="line"></span></a></li>
<li id="nav-menu-item-20931" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#phds" class=""><i class="menu_icon fa blank"></i><span>PhD Students</span><span class="line"></span></a></li>
<li id="nav-menu-item-20936" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#masters" class=""><i class="menu_icon fa blank"></i><span>Master’s and Undergrad Students</span><span class="line"></span></a></li>
<li id="nav-menu-item-20937" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#visitors" class=""><i class="menu_icon fa blank"></i><span>Visiting Researchers</span><span class="line"></span></a></li>
<li id="nav-menu-item-20938" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#alumni" class=""><i class="menu_icon fa blank"></i><span>Alumni</span><span class="line"></span></a></li>
<li id="nav-menu-item-20939" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="index_people.html#jobs" class=""><i class="menu_icon fa blank"></i><span>Job Openings</span><span class="line"></span></a></li>
</ul></div></div>
</li>

<li id="nav-menu-item-20951" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children  has_sub narrow"><a href="" class=""><i class="menu_icon fa blank"></i><span style="color: black">Courses</span><span class="line"></span></a>
<div class="second bellow_header"><div class="inner"><div class="inner_arrow"></div><ul>
<!--<li id="nav-menu-item-20936" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="" target="_blank" class=""><i class="menu_icon fa blank"></i><span>Overview</span><span class="line"></span></a></li>-->
<li id="nav-menu-item-20953" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="https://youtu.be/ZA9tjAiyqa8" target="_blank" class=""><i class="menu_icon fa blank"></i><span>CMSC848J-24Spring-UMD-Cognitive-Robotics</span><span class="line"></span></a></li>
<li id="nav-menu-item-20954" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="https://youtu.be/ZA9tjAiyqa8" target="_blank" class=""><i class="menu_icon fa blank"></i><span>COMP645-25Fall-NCAT-Artificial-Intelligence</span><span class="line"></span></a></li>
</ul></div></div>
</li>

<!--<li id="nav-menu-item-20960" class="menu-item menu-item-type-custom menu-item-object-custom menu-item-has-children  has_sub narrow"><a href="" class=""><i class="menu_icon fa blank"></i><span style="color: black">Photos</span><span class="line"></span></a>-->
<!--<div class="second bellow_header"><div class="inner"><div class="inner_arrow"></div><ul>-->
<!--<li id="nav-menu-item-20961" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="https://yantianzha.github.io/yantianphotos/photography/AAMAS18_Stockholm_Sweden.html" target="_blank" class=""><i class="menu_icon fa blank"></i><span>AAMAS-18_Stockholm_Sweden</span><span class="line"></span></a></li>-->
<!--<li id="nav-menu-item-20962" class="menu-item menu-item-type-custom menu-item-object-custom "><a href="https://yantianzha.github.io/yantianphotos/photography/AAAI19_Honolulu_Hawaii.html" target="_blank" class=""><i class="menu_icon fa blank"></i><span>AAAI-19_Honolulu_Hawaii</span><span class="line"></span></a></li>-->
<!--</ul></div></div>-->
<!--</li>-->

</ul>
</nav>
</div>
</div>
</div>
</header>

            <a id='back_to_top' href='#'>
                <span class="fa-stack">
                    <i class="fa fa-angle-up" style=""></i>
                </span>
            </a>

            <!-- Main Publications Content -->
            <div class="publications-container">
                <div class="publications-header">
                    <h1 class="publications-title">Publications</h1>

                    <!-- Category Filter Section -->
                    <div class="category-filter">
                        <div class="filter-categories">
                            <button class="filter-btn active" data-category="all">Show all</button>
                            <button class="filter-btn" data-category="robotics">Robotics</button>
                            <button class="filter-btn" data-category="perception">Computer Vision</button>
                            <button class="filter-btn" data-category="rl">Reinforcement Learning</button>
                            <button class="filter-btn" data-category="il">Imitation Learning</button>
                            <button class="filter-btn" data-category="hri">Human-Robot Interaction</button>
                            <button class="filter-btn" data-category="planning">Planning</button>
                            <button class="filter-btn" data-category="underwater">Unmanned Underwater Vehicles</button>
                            <button class="filter-btn" data-category="uavs">Unmanned Aerial Vehicles</button>
                            <button class="filter-btn" data-category="simulator">Simulator</button>
                            <button class="filter-btn" data-category="dataset">Dataset</button>
                        </div>
                    </div>
                </div>

                <!-- 2025 Publications -->
                <div class="year-section">
                    <h2 class="year-header">2025</h2>

                    <div class="publication" data-categories="robotics underwater uavs perception dataset simulator rl il">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                                <video autoplay loop muted playsinline width="100%" controls>
                                  <source src="media/videos/AAM-SEALS_All.mp4" type="video/mp4">
                                </video>
                            </div>

                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://arxiv.org/pdf/2412.19744" target="_blank">
                                        AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land Simulator
                                    </a>
                                    <span class="special-designation">ICRA-25 Workshop</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    William Yang, Karthikeya Kona, Yashveer Jain, Abhinav Bhamidipati, <br>Tomer Atzili, Xiaomin Lin, Yantian Zha
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/pdf/2412.19744" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>Paper
                                    </a>
                                    <a href="https://aam-seals.umd.edu/" class="publication-link" target="_blank">
                                        <span class="icon">🌐</span>Website
                                    </a>
                                    <a href="https://drive.google.com/file/d/1wyj_gF96U6ELItr4Ddl8YfpxQvU9X6Ps/view?usp=sharing" class="publication-link" target="_blank">
                                        <span class="icon">🖼️</span>Poster
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Current mobile manipulators and high-fidelity simulators lack the ability to seamlessly operate and simulate across integrated environments spanning sea, air, and land. To address this gap, we introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator (SEALS), a comprehensive and photorealistic simulator designed for AAMs to operate and learn in these diverse environments. The development of AAM-SEALS tackles several significant challenges, including the creation of integrated controllers for flying, swimming, and manipulation, and the high-fidelity simulation of aerial dynamics and hydrodynamics leveraging particle-based hydrodynamics. Our evaluation demonstrates smooth operation and photorealistic transitions across air, water, and their interfaces. We quantitatively validate the fidelity of particle-based hydrodynamics by comparing position-tracking errors across realworld and simulated systems. AAM-SEALS benefits a broad range of robotics communities, including robot learning, aerial robotics, underwater robotics, mobile manipulation, and robotic simulators. We will open-source our code and data to foster the advancement of research in these fields. The overview video is available at <a href="https://youtu.be/MbqIIrYvR78" target="_blank">https://youtu.be/MbqIIrYvR78</a>. Visit our project website at <a href="https://aam-seals.umd.edu" target="_blank">aam-seals.umd.edu</a> for more details.
                                </div>
                            </div>
                        </div>


                    </div>

                    <!-- Example of another publication for comparison -->
                    <div class="publication" data-categories="robotics hri dataset simulator">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                                <iframe
                                    src="https://www.youtube.com/embed/gVx3UEleljk?autoplay=1&mute=1&loop=1&playlist=gVx3UEleljk"
                                    title="AAM-SEALS Demo Video"
                                    frameborder="0"
                                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                    allowfullscreen>
                                </iframe>
                            </div>


                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://arxiv.org/pdf/2412.19744" target="_blank">
                                        NatSGLD: A Dataset with Speech, Gesture, Logic, and Demonstration for Robot Learning in Natural Human-Robot Interaction
                                    </a>
                                    <span class="special-designation">HRI-25</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis Aloimonos, Cornelia Fermüller
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/pdf/2502.16718" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>Paper
                                    </a>
                                    <a href="https://www.snehesh.com/natsgld/" class="publication-link" target="_blank">
                                        <span class="icon">🌐</span>Website
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Recent advances in multimodal Human-Robot Interaction (HRI) datasets emphasize the integration of speech and gestures, allowing robots to absorb explicit knowledge and tacit understanding. However, existing datasets primarily focus on elementary tasks like object pointing and pushing, limiting their applicability to complex domains. They prioritize simpler human command data but place less emphasis on training robots to correctly interpret tasks and respond appropriately. To address these gaps, we present the NatSGLD dataset, which was collected using a Wizard of Oz (WoZ) method, where participants interacted with a robot they believed to be autonomous. NatSGLD records humans’ multimodal commands (speech and gestures), each paired with a demonstration trajectory and a Linear Temporal Logic (LTL) formula that provides a ground-truth interpretation of the commanded tasks. This dataset serves as a foundational resource for research at the intersection of HRI and machine learning. By providing multimodal inputs and detailed annotations, NatSGLD enables exploration in areas such as multimodal instruction following, plan recognition, and human-advisable reinforcement learning from demonstrations. We release the dataset and code under the MIT License at <a href="https://www.snehesh.com/natsgld" target="_blank">https://www.snehesh.com/natsgld</a> to support future HRI research.
                                </div>
                            </div>
                        </div>
                    </div>

                </div>

                <!-- 2024 Publications -->
                <div class="year-section">
                    <h2 class="year-header">2024</h2>

                    <div class="publication" data-categories="robotics perception dataset il">
                        <div class="publication-header">
                            <!-- GIF section -->
                            <div class="publication-video-large" style="display: flex; width: 400px; height: 175px; border-radius: 8px; overflow: hidden;">
                                <!-- Left half: GIF -->
                                <img
                                    src="https://guansuns.github.io/pages/vlm-critic/media/videos/1.gif"
                                    alt="Critic Demo GIF"
                                    style="width: 50%; height: 100%; object-fit: cover;"
                                >

                                <!-- Right half: PNG -->
                                <img
                                    src="https://guansuns.github.io/pages/vlm-critic/media/figures/1.png"
                                    alt="Overlay PNG"
                                    style="width: 50%; height: 90%; object-fit: cover;"
                                >
                            </div>

                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://openreview.net/forum?id=otKo4zFKmH#discussion" target="_blank">
                                        "Task Success" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors
                                    </a>
                                    <span class="special-designation">COLM-24</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben Amor, Subbarao Kambhampati
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://openreview.net/forum?id=otKo4zFKmH#discussion" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>Paper
                                    </a>
                                    <a href="https://guansuns.github.io/pages/vlm-critic/" class="publication-link" target="_blank">
                                        <span class="icon">🌐</span>Website
                                    </a>
<!--                                    <a href="https://drive.google.com/file/d/1wyj_gF96U6ELItr4Ddl8YfpxQvU9X6Ps/view?usp=sharing" class="publication-link" target="_blank">-->
<!--                                        <span class="icon">🖼️</span>Poster-->
<!--                                    </a>-->
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should avoid pointing the blade at a human when handing a knife to the person). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable <b><i>Behavior Critics</i></b> to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement.</div>
                            </div>
                        </div>


                    </div>

                    <!-- Example of another publication for comparison -->
                    <div class="publication" data-categories="robotics rl il">
                        <div class="publication-header">
                            <!-- GIF section -->
                            <div class="publication-video-large">
                                <img
                                    src="https://github.com/YantianZha/SERLfD/raw/main/figures/SERL_gif_3.gif"
                                    alt="SERLfD Demo GIF"
                                    style="width: 100%; height: 100%; object-fit: cover; border-radius: 8px;"
                                >
                            </div>


                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://arxiv.org/pdf/2110.05286" target="_blank">
                                        Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning
                                    </a>
                                    <span class="special-designation">AAAI-24</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Yantian Zha, Lin Guan, Subbarao Kambhampati
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/pdf/2110.05286" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>Paper
                                    </a>
                                    <a href="https://github.com/YantianZha/SERLfD/" class="publication-link" target="_blank">
                                        <span class="icon">🌐</span>Website
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Our work aims at efficiently leveraging ambiguous demonstrations for the training of a reinforcement learning (RL) agent. An ambiguous demonstration can usually be interpreted in multiple ways, which severely hinders the RL agent from learning stably and efficiently. Since an optimal demonstration may also suffer from being ambiguous, previous works that combine RL and learning from demonstration (RLfD works) may not work well. Inspired by how humans handle such situations, we propose to use self-explanation (an agent generates explanations for itself) to recognize valuable high-level relational features as an interpretation of why a successful trajectory is successful. This way, the agent can leverage the explained important relations as guidance for its RL learning. Our main contribution is to propose the SelfExplanation for RL from Demonstrations (SERLfD) framework, which can overcome the limitations of existing RLfD works. Our experimental results show that an RLfD model can be improved by using our SERLfD framework in terms of training stability and performance. To foster further research in self-explanation-guided robot learning, we have made our demonstrations and code publicly accessible at <a href="https://github.com/YantianZha/SERLfD" target="_blank">https://github.com/YantianZha/SERLfD</a>.
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- 2022 Publications -->
                <div class="year-section">
                    <h2 class="year-header">2022</h2>

                    <div class="publication" data-categories="robotics hri">
                        <div class="publication-header">
                            <!-- GIF section -->
                            <div class="publication-video-large" style="display: flex; width: 400px; height: 175px; border-radius: 8px; overflow: hidden;">

                            </div>

                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21488/21237" target="_blank">
                                        Symbols as a Lingua Franca for Bridging Human-AI Chasm for Explainable and Advisable AI Systems
                                    </a>
                                    <span class="special-designation">AAAI-22</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Subbarao Kambhampati, Sarath Sreedharan, Mudit Verma, Yantian Zha, Lin Guan
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/pdf/2109.09904" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>arXiv
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Despite the surprising power of many modern AI systems that often learn their own representations, there is significant discontent about their inscrutability and the attendant problems in their ability to interact with humans. While alternatives such as neuro-symbolic approaches have been proposed, there is a lack of consensus on what they are about. There are often two independent motivations (i) symbols as a lingua franca for human-AI interaction and (ii) symbols as systemproduced abstractions used by the AI system in its internal reasoning. The jury is still out on whether AI systems will need to use symbols in their internal reasoning to achieve general intelligence capabilities. Whatever the answer there is, the need for (human-understandable) symbols in human-AI interaction seems quite compelling. Symbols, like emotions, may well not be sine qua non for intelligence per se, but they will be crucial for AI systems to interact with us humans – as we can neither turn off our emotions nor get by without our symbols. In particular, in many human-designed domains, humans would be interested in providing explicit (symbolic) knowledge and advice – and expect machine explanations in kind. This alone requires AI systems to to maintain a symbolic interface for interaction with humans. In this blue sky paper, we argue this point of view, and discuss research directions that need to be pursued to allow for this type of human-AI interaction.
                                </div>
                        </div>
                    </div>
                </div>

                <!-- 2021 Publications -->
                <div class="year-section">
                    <h2 class="year-header">2021</h2>

                    <div class="publication" data-categories="robotics perception il">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                                <iframe
                                    src="https://www.youtube.com/embed/Y_4u72Wm3Tk?autoplay=1&mute=1&loop=1&playlist=Y_4u72Wm3Tk"
                                    title="Affordance Demo Video"
                                    frameborder="0"
                                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                    allowfullscreen>
                                </iframe>
                            </div>

                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://drive.google.com/file/d/1NjTrwPTFbpvktXgQRivUFIH4GlB4Ksr3/view" target="_blank">
                                        Contrastively Learning Visual Attention as Affordance Cues from Demonstrations for Robotic Grasping
                                    </a>
                                    <span class="special-designation">IROS-21</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Yantian Zha, Siddhant Bhambri, Lin Guan
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/abs/2104.00878" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>arXiv
                                    </a>
                                    <a href="https://sites.google.com/asu.edu/affordance-aware-imitation/project/" class="publication-link" target="_blank">
                                        <span class="icon">🌐</span>Website
                                    </a>
<!--                                    <a href="https://drive.google.com/file/d/1wyj_gF96U6ELItr4Ddl8YfpxQvU9X6Ps/view?usp=sharing" class="publication-link" target="_blank">-->
<!--                                        <span class="icon">🖼️</span>Poster-->
<!--                                    </a>-->
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Conventional works that learn grasping affordance from demonstrations need to explicitly predict grasping configurations, such as gripper approaching angles or grasping preshapes. Classic motion planners could then sample trajectories by using such predicted configurations. In this work,our goal is instead to fill the gap between affordance discovery and affordance-based policy learning by integrating the two objectives in an end-to-end imitation learning framework based on deep neural networks. From a psychological perspective,there is a close association between attention and affordance.Therefore, with an end-to-end neural network, we propose to learn affordance cues as visual attention that serves as a useful indicating signal of how a demonstrator accomplishes tasks,instead of explicitly modeling affordances. To achieve this, we propose a contrastive learning framework that consists of a Siamese encoder and a trajectory decoder. We further introduce a coupled triplet loss to encourage the discovered affordance cues to be more affordance-relevant. Our experimental results demonstrate that our model with the coupled triplet loss achieves the highest grasping success rate in a simulated robot environment.
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- 2019 Publications -->
                <div class="year-section">
                    <h2 class="year-header">2019</h2>

                    <div class="publication" data-categories="robotics planning hri">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                                <iframe
                                    src="https://www.youtube.com/embed/iLG-ANQtYms?autoplay=1&mute=1&loop=1&playlist=iLG-ANQtYms"
                                    title="AAM-SEALS Demo Video"
                                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                    allowfullscreen>
                                </iframe>
                            </div>

                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://dl.acm.org/doi/10.5555/3306127.3332015" target="_blank">
                                        Explicability as Minimizing Distance from Expected Behavior
                                    </a>
                                    <span class="special-designation">AAMAS-19</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam Vadlamudi, Yu Zhang and Subbarao Kambhampati
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/abs/1611.05497" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>arXiv
                                    </a>
<!--                                    <a href="https://aam-seals.umd.edu/" class="publication-link" target="_blank">-->
<!--                                        <span class="icon">🌐</span>Website-->
<!--                                    </a>-->
<!--                                    <a href="https://drive.google.com/file/d/1wyj_gF96U6ELItr4Ddl8YfpxQvU9X6Ps/view?usp=sharing" class="publication-link" target="_blank">-->
<!--                                        <span class="icon">🖼️</span>Poster-->
<!--                                    </a>-->
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    In order to achieve effective human-AI collaboration, it is necessary for an AI agent to align its behavior with the human's expectations. When the agent generates a task plan without such considerations, it may often result in inexplicable behavior from the human's point of view. This may have serious implications for the human, from increased cognitive load to more serious concerns of safety around the physical agent. In this work, we present an approach to generate explicable behavior by minimizing the distance between the agent's plan and the plan expected by the human. To this end, we learn a mapping between plan distances (distances between expected and agent plans) and human's plan scoring scheme. The plan generation process uses this learned model as a heuristic. We demonstrate the effectiveness of our approach in a delivery robot domain.
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Example of another publication for comparison -->
                    <div class="publication" data-categories="planning">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                            </div>


                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://dl.acm.org/doi/10.1145/3368270" target="_blank">
                                        Discovering Underlying Plans Based on Shallow Models
                                    </a>
                                    <span class="special-designation">ACM-TIST-19</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Hankz Hankui Zhuo, Yantian Zha, Subbarao Kambhampati, and Xin Tian
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/abs/1812.00301" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>arXiv
                                    </a>
                                    <a href="https://github.com/YantianZha/Discovering-Underlying-Plans-Based-on-Shallow-Models" class="publication-link" target="_blank">
                                        <span class="icon">💻</span>Code
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or action models in hand. Previous approaches either discover plans by maximally “matching” observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing action models to best explain the observed actions, assuming that complete action models are available. In real world applications, however, target plans are often not from plan libraries, and complete action models are often not available, since building complete sets of plans and complete action models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Specifically, we propose two approaches, DUP and RNNPlanner, to discover target plans based on vector representations of actions. DUP explores the EM-style (Expectation Maximization) framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while RNNPlanner aims to leverage long-short term contexts of actions based on RNNs (recurrent neural networks) framework to help recognize target plans. In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries, without requiring action models provided. We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains. We also compare DUP and RNNPlanner to see their advantages and disadvantages.
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Example of another publication for comparison -->
                    <div class="publication" data-categories="perception planning">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                            </div>


                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://arxiv.org/abs/1812.00301" target="_blank">
                                        Plan-Recognition-Driven Attention Modeling for Visual Recognition
                                    </a>
                                    <span class="special-designation">AAAI-19 Workshop</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Yantian Zha, Yikang Li, Tianshu Yu, Subbarao Kambhampati, Baoxin Li
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://yochan-lab.github.io/papers/files/papers/hankz_tist_19.pdf" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>arXiv
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Human visual recognition of activities or external agents involves an interplay between high-level plan recognition and low-level perception. Given that, a natural question to ask is: can low-level perception be improved by high-level plan recognition? We formulate the problem of leveraging recognized plans to generate better top-down attention maps \cite{gazzaniga2009,baluch2011} to improve the perception performance. We call these top-down attention maps specifically as plan-recognition-driven attention maps. To address this problem, we introduce the Pixel Dynamics Network. Pixel Dynamics Network serves as an observation model, which predicts next states of object points at each pixel location given observation of pixels and pixel-level action feature. This is like internally learning a pixel-level dynamics model. Pixel Dynamics Network is a kind of Convolutional Neural Network (ConvNet), with specially-designed architecture. Therefore, Pixel Dynamics Network could take the advantage of parallel computation of ConvNets, while learning the pixel-level dynamics model. We further prove the equivalence between Pixel Dynamics Network as an observation model, and the belief update in partially observable Markov decision process (POMDP) framework. We evaluate our Pixel Dynamics Network in event recognition tasks. We build an event recognition system, ER-PRN, which takes Pixel Dynamics Network as a subroutine, to recognize events based on observations augmented by plan-recognition-driven attention.
                                </div>
                            </div>
                        </div>
                    </div>

                </div>

                <!-- 2018 Publications -->
                <div class="year-section">
                    <h2 class="year-header">2018</h2>

                    <div class="publication" data-categories="perception planning">
                        <div class="publication-header">
                            <!-- Video section with proper size -->
                            <div class="publication-video-large">
                                <img
                                src="media/figures/Distr2Vec.png"
                                alt="Distr2Vec Image"
                                style="width:100%; height:100%; display:block; margin:0 auto;">

                            </div>

                            <!-- Text section with readable font sizes -->
                            <div class="publication-text">
                                <h3 class="publication-title">
                                    <a href="https://dl.acm.org/doi/10.5555/3237383.3238103" target="_blank">
                                        Recognizing Plans by Learning Embeddings from Observed Action Distributions
                                    </a>
                                    <span class="special-designation">AAMAS-18</span>
                                </h3>
<!--                                <div class="publication-venue">-->
<!--                                    arXiv, 2024; ICRA Amphibious Robotics Workshop, 2025-->
<!--                                </div>-->
                                <div class="publication-authors">
                                    Yantian Zha, Yikang Li, Sriram Gopalakrishnan, Baoxin Li, and Subbarao Kambhampati
                                </div>

                                <!-- Links and abstract -->
                                <div class="publication-links">
                                    <a href="https://arxiv.org/abs/1712.01949" class="publication-link" target="_blank">
                                        <span class="icon">📄</span>arXiv
                                    </a>
                                    <a href="https://github.com/YantianZha/Distr2Vec" class="publication-link" target="_blank">
                                        <span class="icon">💻</span>Code
                                    </a>
                                </div>
                                <div class="show-abstract" onclick="toggleAbstract(this)">Show Abstract</div>
                                <div class="publication-abstract">
                                    Automated video surveillance requires the recognition of agent plans from videos. One promising direction for plan recognition involves learning shallow action affinity models from plan traces. Extracting such traces from raw video involves uncertainty about the actions. One solution is to represent traces as sequences of action distributions. To use such a representation in approximate plan recognition, we need embeddings of these action distributions. To address this problem, we propose a distribution to vector (Distr2Vec) model, which learns embeddings of action distributions using KL-divergence as the loss function.
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>

    <script>
        function toggleAbstract(element) {
            const abstract = element.nextElementSibling;
            const isVisible = abstract.style.display === 'block';

            if (isVisible) {
                abstract.style.display = 'none';
                element.textContent = 'Show Abstract';
            } else {
                abstract.style.display = 'block';
                element.textContent = 'Hide Abstract';
            }
        }

        // Category filtering functionality
        document.addEventListener('DOMContentLoaded', function() {
            const filterBtns = document.querySelectorAll('.filter-btn');
            const publications = document.querySelectorAll('.publication');

            filterBtns.forEach(btn => {
                btn.addEventListener('click', function() {
                    // Remove active class from all buttons
                    filterBtns.forEach(b => b.classList.remove('active'));
                    // Add active class to clicked button
                    this.classList.add('active');

                    const category = this.getAttribute('data-category');

                    publications.forEach(pub => {
                        if (category === 'all') {
                            pub.style.display = 'block';
                        } else {
                            const categories = pub.getAttribute('data-categories');
                            if (categories && categories.includes(category)) {
                                pub.style.display = 'block';
                            } else {
                                pub.style.display = 'none';
                            }
                        }
                    });
                });
            });
        });
    </script>




            <!-- Footer -->
            <footer>
                <div class="vc_row wpb_row section grid_section" style='padding-top:44px; padding-bottom:24px; text-align:left;'>
                    <div class="section_inner clearfix">
                        <div class="abstract">
                            <p>Website designed by Yantian Zha</p>
                            <p>Contact: yzha@ncat.edu</p>
                        </div>
                    </div>
                </div>
            </footer>
        </div>
    </div>

    <!-- Scripts -->
    <script type="text/javascript" src="./static/js/FromTiger/qode-like.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/plugins.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/jquery.carouFredSel-6.2.1.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/jquery.mousewheel.min.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/jquery.touchSwipe.min.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/default_dynamic.php.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/default.min.js"></script>
    <script type="text/javascript" src="./static/js/FromTiger/custom_js.php.js"></script>

    <script>
        // Category filtering functionality
        function initializeFiltering() {
            const filterButtons = document.querySelectorAll('.filter-btn');
            const publications = document.querySelectorAll('.publication');

            filterButtons.forEach(button => {
                button.addEventListener('click', function() {
                    // Remove active class from all buttons
                    filterButtons.forEach(btn => btn.classList.remove('active'));
                    // Add active class to clicked button
                    this.classList.add('active');

                    const selectedCategory = this.getAttribute('data-category');

                    publications.forEach(publication => {
                        const categories = publication.getAttribute('data-categories');

                        if (selectedCategory === 'all' || categories.includes(selectedCategory)) {
                            publication.style.display = 'block';
                            // Add fade-in animation
                            publication.style.opacity = '0';
                            setTimeout(() => {
                                publication.style.opacity = '1';
                            }, 100);
                        } else {
                            publication.style.display = 'none';
                        }
                    });
                });
            });
        }

        // Function to toggle abstract visibility
        function toggleAbstract(element) {
            const abstract = element.nextElementSibling;
            const isVisible = abstract.style.display === 'block';

            if (isVisible) {
                abstract.style.display = 'none';
                element.textContent = 'Show Abstract';
            } else {
                abstract.style.display = 'block';
                element.textContent = 'Hide Abstract';
            }
        }

        // Initialize everything when page loads
        document.addEventListener('DOMContentLoaded', function() {
            initializeFiltering();
        });

        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Back to top functionality
        const backToTop = document.getElementById('back_to_top');
        window.addEventListener('scroll', function() {
            if (window.pageYOffset > 300) {
                backToTop.style.display = 'block';
            } else {
                backToTop.style.display = 'none';
            }
        });

        backToTop.addEventListener('click', function(e) {
            e.preventDefault();
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });
    </script>
</body>
</html>